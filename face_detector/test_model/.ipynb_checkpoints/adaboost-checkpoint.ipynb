{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac1cdcf-8491-4879-bb36-c213d0119391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nbimporter\n",
    "from weak_classifier import DecisionStump\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from features import eval_feature\n",
    "from Intergral import process_image_data\n",
    "\n",
    "def adaboost_model(feature_values, labels, max_stumps):\n",
    "    n_features, n_samples = feature_values.shape\n",
    "    y = np.copy(labels)\n",
    "    y[y == 0] = -1  # convert 0â†’-1 for AdaBoost math\n",
    "\n",
    "    # init weights\n",
    "    pos = np.sum(y == 1)\n",
    "    neg = np.sum(y == -1)\n",
    "    w = np.zeros(n_samples)\n",
    "    w[y == 1] = 1.0 / (2 * pos)\n",
    "    w[y == -1] = 1.0 / (2 * neg)\n",
    "    w /= w.sum()\n",
    "\n",
    "    classifiers = []\n",
    "\n",
    "    for t in tqdm(range(max_stumps), desc=\"Training maximun num_stumps\"):\n",
    "        best = None\n",
    "\n",
    "        for f in tqdm(range(n_features), desc=f\"Training stumps (Stump {t+1})\", leave=False):\n",
    "            vals = feature_values[f]\n",
    "            order = np.argsort(vals)\n",
    "            sorted_vals, sorted_y, sorted_w = vals[order], y[order], w[order]\n",
    "\n",
    "            S_pos = (sorted_w * (sorted_y == 1)).cumsum()\n",
    "            S_neg = (sorted_w * (sorted_y == -1)).cumsum()\n",
    "            T_pos, T_neg = S_pos[-1], S_neg[-1]\n",
    "\n",
    "            errors1 = S_pos + (T_neg - S_neg) # Error for decide that non-face on the left and face on the right\n",
    "            errors2 = S_neg + (T_pos - S_pos) # Error for decide that face on the left and non-face on the right\n",
    "            errs = np.minimum(errors1, errors2)\n",
    "            min_idx = np.argmin(errs)\n",
    "            err = errs[min_idx]\n",
    "\n",
    "            thr = sorted_vals[min_idx]\n",
    "            pol = -1 if errors1[min_idx] < errors2[min_idx] else 1\n",
    "            best = (f, thr, pol)\n",
    "                \n",
    "        aos = 0.5 * np.log((1 - err + 1e-10) / (err + 1e-10))\n",
    "        f_idx, thr, pol = best\n",
    "\n",
    "        stump = DecisionStump(f_idx, thr, pol, aos)\n",
    "        preds = stump.predict(feature_values[f_idx])\n",
    "        w *= np.exp(-aos * y * preds)\n",
    "        w /= w.sum()\n",
    "\n",
    "        classifiers.append(stump)\n",
    "\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f67df06-b837-4a0a-a684-2892fe9e6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adaboost(pos_imgs, neg_imgs, features, max_stumps, cache_path=\"./cache/adaboost_stumps.pkl\"):\n",
    "    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"[INFO] Loading trained AdaBoost from '{cache_path}' ...\")\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            classifiers = pickle.load(f)\n",
    "        print(f\"[INFO] Loaded AdaBoost with {len(classifiers)} stumps.\")\n",
    "        return classifiers\n",
    "\n",
    "    print(\"Preparing Integral Matrix .....\")\n",
    "    pos_iis, pos_siis = zip(*[\n",
    "        process_image_data(im) for im in tqdm(pos_imgs, desc=\"Processing Positive Images\")\n",
    "    ])\n",
    "    neg_iis, neg_siis = zip(*[\n",
    "        process_image_data(im) for im in tqdm(neg_imgs, desc=\"Processing Negative Images\")\n",
    "    ])\n",
    "\n",
    "    all_iis = pos_iis + neg_iis\n",
    "    all_siis = pos_siis + neg_siis\n",
    "    labels = np.hstack([np.ones(len(pos_iis)), np.zeros(len(neg_iis))])\n",
    "\n",
    "    indices = np.arange(len(all_iis))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    all_iis = [all_iis[i] for i in indices]\n",
    "    all_siis = [all_siis[i] for i in indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    print(\"Computing Haar feature values .....\")\n",
    "    fv = np.zeros((len(features), len(all_iis)), dtype=np.float32)\n",
    "    for i, f in enumerate(tqdm(features, desc=\"Evaluating features\")):\n",
    "        for j, (ii_j, sii_j) in enumerate(zip(all_iis, all_siis)):\n",
    "            fv[i, j] = eval_feature(ii_j, sii_j, f)\n",
    "\n",
    "    print(f\"[INFO] Training AdaBoost with {len(features)} features and {len(all_iis)} samples ...\")\n",
    "    classifiers = adaboost_model(fv, labels, max_stumps)\n",
    "\n",
    "    with open(cache_path, \"wb\") as f:\n",
    "        pickle.dump(classifiers, f)\n",
    "    print(f\"[INFO] AdaBoost model saved to '{cache_path}'\")\n",
    "\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17a739-f30f-427e-9c42-241861b32304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
